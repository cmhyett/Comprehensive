%\todo{Misha/Daniel}
The velocity gradient tensor (VGT) describes many important aspects of turbulence. It displays characteristic non-Gaussian statistics including intermittency, describes the deformation rate of a fluid volume, and encapsulates alignment between strain and vorticity \cite{meneveau2011lagrangian}. The VGT has been the subject of much study and with the exponential growth of computation, direct numerical simulations that resolve the smallest scales of turbulence have provided enormous VGT datasets. Even more recently, machine learning (ML), particularly physics-informed machine learning (PIML), have shown remarkable ability to glean predictive capability from these large datasets, suggesting patterns exist that we have not yet recognized.

Of particular note in recent years is the Recent Deformation of Gaussian Fields model (RDGF) \cite{johnson2016}, wherein the authors postulate a closure of the VGT evolution equations via choosing a Gaussian upstream condition that is deformed according to the current VGT. This hypothesis is in the same spirit as the Tetrad model from Chertkov et.al \cite{chertkov1999} that proposed a local closure to the VGT equations, by closing the equations for the VGT using deformation of a small fluid element. 

In the realm of machine learning, this work is inspired by Tian et.al\cite{tian2021}, that used a highly structured, so-called Tensor Basis Neural Network (TBNN) architecture to close the equations. This network structure was inspired by theoretical work by \cite{pope1975}, \cite{lawson2015}, \cite{johnson2016}. It has been shown to explicitly respect Galilean and rotational invariance, enforce incompressibility, and after training, can predict many quantities of interest in the statistics of the VGT.

Our goals in this paper are to advance the phenomenology via data analysis, and leverage this to improve the TBNN methodology to better predict the statistical evolution of the VGT.