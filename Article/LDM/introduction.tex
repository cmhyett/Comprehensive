Turbulence being a widespread phenomenon, an enormous body of work is dedicated to modeling and simulating it. Resulting from this plethora of simulation efforts are enormous datasets. Recently, with the advances of machine learning - particularly physics-informed machine learning - data-driven models regularly out-perform phenomenological models, suggesting that these large datasets contain exploitable patterns that we don't yet recognize.

At the same time, the promise of physics-informed machine learning is that via imposing structure on the infuriatingly flexible neural network, one might simultaneously improve generalizability (i.e., learn not just data patterns, but physical laws), and have those learned models be interpretable.

Our work focuses on the Lagrangian perspective of turbulence, centering ourselves on the statistical evolution of the velocity gradient tensor (VGT). Recent work \cite{tian2021} constructed the so called ``Tensor Basis Neural Network'' (TBNN), and trained it to learn the pressure Hessian contribution to the evolution of the VGT.

In this work we will present the TBNN formulation, reiterate the results of Tian et.al, and show that by examining the trained model, one can tease out physically relevant patterns, toward the hope of learning new physics.
